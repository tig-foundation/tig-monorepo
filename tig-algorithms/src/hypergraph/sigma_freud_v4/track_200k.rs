use cudarc::{
    driver::{safe::LaunchConfig, CudaModule, CudaStream, PushKernelArg}, 
    runtime::sys::cudaDeviceProp,
};
use serde_json::{Map, Value};
use std::sync::Arc;
use tig_challenges::hypergraph::*;

pub fn solve(
    challenge: &Challenge,
    save_solution: &dyn Fn(&Solution) -> anyhow::Result<()>,
    hyperparameters: &Option<Map<String, Value>>,
    module: Arc<CudaModule>,
    stream: Arc<CudaStream>,
    prop: &cudaDeviceProp,
) -> anyhow::Result<()> {
    let block_size = std::cmp::min(128, prop.maxThreadsPerBlock as u32);

    let hyperedge_cluster_kernel = module.load_function("hyperedge_clustering")?;
    let compute_preferences_kernel = module.load_function("compute_node_preferences")?;
    let execute_assignments_kernel = module.load_function("execute_node_assignments")?;
    let precompute_edge_flags_kernel = module.load_function("precompute_edge_flags")?;
    let compute_moves_kernel = module.load_function("compute_refinement_moves_optimized")?;
    let execute_moves_kernel = module.load_function("execute_refinement_moves")?;
    let balance_kernel = module.load_function("balance_final")?;
    let reset_counters_kernel = module.load_function("reset_counters")?;
    let compute_connectivity_kernel = module.load_function("compute_connectivity")?;
    let perturb_kernel = module.load_function("perturb_solution")?;

    let cfg = LaunchConfig {
        grid_dim: (
            (challenge.num_nodes as u32 + block_size - 1) / block_size,
            1,
            1,
        ),
        block_dim: (block_size, 1, 1),
        shared_mem_bytes: 0,
    };

    let one_thread_cfg = LaunchConfig {
        grid_dim: (1, 1, 1),
        block_dim: (1, 1, 1),
        shared_mem_bytes: 0,
    };

    let hedge_cfg = LaunchConfig {
        grid_dim: (
            (challenge.num_hyperedges as u32 + block_size - 1) / block_size,
            1,
            1,
        ),
        block_dim: (block_size, 1, 1),
        shared_mem_bytes: 0,
    };

    let mut num_hedge_clusters = if let Some(params) = hyperparameters {
        params
            .get("clusters")
            .and_then(|v| v.as_i64())
            .map(|v| v.clamp(4, 256) as i32)
            .unwrap_or(64)
    } else {
        64
    };

    if num_hedge_clusters % 4 != 0 {
        num_hedge_clusters += 4 - (num_hedge_clusters % 4);
    }

    let mut d_hyperedge_clusters = stream.alloc_zeros::<i32>(challenge.num_hyperedges as usize)?;
    let mut d_partition = stream.alloc_zeros::<i32>(challenge.num_nodes as usize)?;
    let mut d_nodes_in_part = stream.alloc_zeros::<i32>(challenge.num_parts as usize)?;

    let mut d_pref_parts = stream.alloc_zeros::<i32>(challenge.num_nodes as usize)?;
    let mut d_pref_priorities = stream.alloc_zeros::<i32>(challenge.num_nodes as usize)?;
   
    let mut d_move_priorities = stream.alloc_zeros::<i32>(challenge.num_nodes as usize)?;

    let mut d_num_valid_moves = stream.alloc_zeros::<i32>(1)?;
    let mut d_moves_executed = stream.alloc_zeros::<i32>(1)?;

    let mut d_edge_flags_all = stream.alloc_zeros::<u64>(challenge.num_hyperedges as usize)?;
    let mut d_edge_flags_double = stream.alloc_zeros::<u64>(challenge.num_hyperedges as usize)?;

    let is_sparse = (challenge.num_nodes as usize) > 4 * (challenge.num_hyperedges as usize + 1);

    let effort = hyperparameters
        .as_ref()
        .and_then(|p| p.get("effort").and_then(|v| v.as_i64()))
        .unwrap_or(2);

    let (base_refine, base_ils, base_ils_quick, base_polish, base_post_balance) = match effort {
        5 => (500, 5, 50, 150, 64),
        4 => (500, 5, 45, 120, 55),
        3 => (500, 4, 38, 85, 45),
        2 => (450, 4, 30, 60, 35),
        1 => (400, 3, 25, 40, 25),
        0 => (300, 3, 20, 30, 20),
        _ => (450, 4, 30, 60, 35),
    };

    let refinement_rounds = hyperparameters
        .as_ref()
        .and_then(|p| p.get("refinement").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(50, 5000) as usize)
        .unwrap_or(base_refine);

    let ils_iterations = hyperparameters
        .as_ref()
        .and_then(|p| p.get("ils_iterations").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(1, 10) as usize)
        .unwrap_or(base_ils);

    let ils_quick_refine = hyperparameters
        .as_ref()
        .and_then(|p| p.get("ils_quick_refine").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(10, 100) as usize)
        .unwrap_or(base_ils_quick);

    let post_ils_polish = hyperparameters
        .as_ref()
        .and_then(|p| p.get("post_ils_polish").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(20, 200) as usize)
        .unwrap_or(base_polish);

    let move_limit: usize = hyperparameters
        .as_ref()
        .and_then(|p| p.get("move_limit").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(256, 1_000_000) as usize)
        .unwrap_or(if is_sparse {
            262_144
        } else if challenge.num_hyperedges as usize >= 150_000 || challenge.num_nodes as usize >= 250_000 {
            131_072
        } else {
            200_000
        });

    unsafe {
        stream
            .launch_builder(&hyperedge_cluster_kernel)
            .arg(&(challenge.num_hyperedges as i32))
            .arg(&(num_hedge_clusters as i32))
            .arg(&challenge.d_hyperedge_offsets)
            .arg(&challenge.d_hyperedge_nodes)
            .arg(&mut d_hyperedge_clusters)
            .launch(LaunchConfig {
                grid_dim: (
                    (challenge.num_hyperedges as u32 + block_size - 1) / block_size,
                    1,
                    1,
                ),
                block_dim: (block_size, 1, 1),
                shared_mem_bytes: 0,
            })?;
    }

    unsafe {
        stream
            .launch_builder(&compute_preferences_kernel)
            .arg(&(challenge.num_nodes as i32))
            .arg(&(challenge.num_parts as i32))
            .arg(&(num_hedge_clusters as i32))
            .arg(&challenge.d_node_hyperedges)
            .arg(&challenge.d_node_offsets)
            .arg(&d_hyperedge_clusters)
            .arg(&challenge.d_hyperedge_offsets)
            .arg(&mut d_pref_parts)
            .arg(&mut d_pref_priorities)
            .launch(cfg.clone())?;
    }

    let pref_parts = stream.memcpy_dtov(&d_pref_parts)?;
    let pref_priorities = stream.memcpy_dtov(&d_pref_priorities)?;

    let mut indices: Vec<usize> = (0..challenge.num_nodes as usize).collect();
    indices.sort_unstable_by(|&a, &b| {
        pref_priorities[b].cmp(&pref_priorities[a]).then_with(|| a.cmp(&b))
    });

    let sorted_nodes: Vec<i32> = indices.iter().map(|&i| i as i32).collect();
    let sorted_parts: Vec<i32> = indices.iter().map(|&i| pref_parts[i]).collect();

    let d_sorted_nodes = stream.memcpy_stod(&sorted_nodes)?;
    let d_sorted_parts = stream.memcpy_stod(&sorted_parts)?;

    unsafe {
        stream
            .launch_builder(&execute_assignments_kernel)
            .arg(&(challenge.num_nodes as i32))
            .arg(&(challenge.num_parts as i32))
            .arg(&(challenge.max_part_size as i32))
            .arg(&d_sorted_nodes)
            .arg(&d_sorted_parts)
            .arg(&mut d_partition)
            .arg(&mut d_nodes_in_part)
            .launch(one_thread_cfg.clone())?;
    }

    
    let mut sorted_move_nodes: Vec<i32> = Vec::with_capacity(challenge.num_nodes as usize);
    let mut sorted_move_parts: Vec<i32> = Vec::with_capacity(challenge.num_nodes as usize);

    let mut valid_moves: Vec<(usize, i32)> = Vec::with_capacity(challenge.num_nodes as usize);

    let mut stagnant_rounds = 0;
    let _early_exit_round = 100;
    let max_stagnant_rounds = 30;
    
    
    let tabu_tenure = 8;
    let mut node_tabu_until: Vec<usize> = vec![0; challenge.num_nodes as usize];

    for round in 0..refinement_rounds {
        unsafe {
            stream
                .launch_builder(&reset_counters_kernel)
                .arg(&mut d_num_valid_moves)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }

        unsafe {
            stream
                .launch_builder(&precompute_edge_flags_kernel)
                .arg(&(challenge.num_hyperedges as i32))
                .arg(&(challenge.num_nodes as i32))
                .arg(&challenge.d_hyperedge_nodes)
                .arg(&challenge.d_hyperedge_offsets)
                .arg(&d_partition)
                .arg(&mut d_edge_flags_all)
                .arg(&mut d_edge_flags_double)
                .launch(hedge_cfg.clone())?;
        }

        unsafe {
            stream
                .launch_builder(&compute_moves_kernel)
                .arg(&(challenge.num_nodes as i32))
                .arg(&(challenge.num_parts as i32))
                .arg(&(challenge.max_part_size as i32))
                .arg(&challenge.d_node_hyperedges)
                .arg(&challenge.d_node_offsets)
                .arg(&d_partition)
                .arg(&d_nodes_in_part)
                .arg(&d_edge_flags_all)
                .arg(&d_edge_flags_double)
                .arg(&mut d_move_priorities)
                .arg(&mut d_num_valid_moves)
                .launch(cfg.clone())?;
        }
        let num_valid_moves = stream.memcpy_dtov(&d_num_valid_moves)?[0];
        if num_valid_moves == 0 {
            break;
        }

        let move_keys = stream.memcpy_dtov(&d_move_priorities)?;

        valid_moves.clear();
        
        let max_gain = move_keys.iter().map(|&k| k >> 16).max().unwrap_or(0);
        let aspiration_threshold = (max_gain * 3) / 4;  
        
        for (node, &key) in move_keys.iter().enumerate() {
            if key > 0 {
                let gain = key >> 16;
                
                if node_tabu_until[node] <= round || gain >= aspiration_threshold {
                    valid_moves.push((node, key));
                }
            }
        }

        if valid_moves.is_empty() {
            break;
        }

        let cmp = |a: &(usize, i32), b: &(usize, i32)| b.1.cmp(&a.1).then(a.0.cmp(&b.0));

        let mut k = valid_moves.len();
        
        
        let adaptive_limit = if round < 50 {
            
            move_limit / 2
        } else if round < 200 {
            
            move_limit
        } else {
            
            move_limit / 3
        };
        
        if k > adaptive_limit {
            k = adaptive_limit;
            valid_moves.select_nth_unstable_by(k - 1, cmp);
            valid_moves[..k].sort_unstable_by(cmp);
        } else if k > 1000 {
            
            valid_moves.select_nth_unstable_by(k - 1, cmp);
            valid_moves[..k].sort_unstable_by(cmp);
        } else {
            
            valid_moves.sort_unstable_by(cmp);
        }

        sorted_move_nodes.clear();
        sorted_move_parts.clear();
        sorted_move_nodes.extend(valid_moves[..k].iter().map(|(node, _)| *node as i32));
        sorted_move_parts.extend(valid_moves[..k].iter().map(|(_, key)| (key & 63) as i32));

        let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
        let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;

        unsafe {
            stream
                .launch_builder(&execute_moves_kernel)
                .arg(&(sorted_move_nodes.len() as i32))
                .arg(&d_sorted_move_nodes)
                .arg(&d_sorted_move_parts)
                .arg(&(challenge.max_part_size as i32))
                .arg(&mut d_partition)
                .arg(&mut d_nodes_in_part)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }
        let mut moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];

        if moves_executed == 0 && k < valid_moves.len() {
            
            sorted_move_nodes.clear();
            sorted_move_parts.clear();
            for i in k..valid_moves.len() {
                sorted_move_nodes.push(valid_moves[i].0 as i32);
                sorted_move_parts.push((valid_moves[i].1 & 63) as i32);
            }

            let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
            let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;

            unsafe {
                stream
                    .launch_builder(&execute_moves_kernel)
                    .arg(&(sorted_move_nodes.len() as i32))
                    .arg(&d_sorted_move_nodes)
                    .arg(&d_sorted_move_parts)
                    .arg(&(challenge.max_part_size as i32))
                    .arg(&mut d_partition)
                    .arg(&mut d_nodes_in_part)
                    .arg(&mut d_moves_executed)
                    .launch(one_thread_cfg.clone())?;
            }

            moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];
        }
        
        
        if moves_executed > 0 {
            for &node in sorted_move_nodes.iter().take(moves_executed as usize) {
                node_tabu_until[node as usize] = round + tabu_tenure;
            }
        }

        if moves_executed == 0 {
            stagnant_rounds += 1;
            
            
            if stagnant_rounds >= 5 && round < refinement_rounds - 50 {
                
                let mini_seed = 987654321u64 + (round as u64) * 123456789u64;
                unsafe {
                    stream
                        .launch_builder(&perturb_kernel)
                        .arg(&(challenge.num_nodes as i32))
                        .arg(&(challenge.num_parts as i32))
                        .arg(&(challenge.max_part_size as i32))
                        .arg(&1i32)  
                        .arg(&mut d_partition)
                        .arg(&mut d_nodes_in_part)
                        .arg(&mini_seed)
                        .launch(one_thread_cfg.clone())?;
                }
                stagnant_rounds = 0;  
            } else if stagnant_rounds > max_stagnant_rounds {
                break;  
            }
        } else {
            stagnant_rounds = 0;
        }
    }

    
    let perturb_strength = 1; 
    
    let mut d_connectivity = stream.alloc_zeros::<i32>(challenge.num_hyperedges as usize)?;
    
    
    unsafe {
        stream
            .launch_builder(&compute_connectivity_kernel)
            .arg(&(challenge.num_hyperedges as i32))
            .arg(&challenge.d_hyperedge_nodes)
            .arg(&challenge.d_hyperedge_offsets)
            .arg(&d_partition)
            .arg(&mut d_connectivity)
            .launch(hedge_cfg.clone())?;
    }
    
    let connectivity_vec = stream.memcpy_dtov(&d_connectivity)?;
    let mut best_connectivity: i32 = connectivity_vec.iter().sum();
    
    
    let mut best_partition_host = stream.memcpy_dtov(&d_partition)?;
    let mut best_nodes_in_part_host = stream.memcpy_dtov(&d_nodes_in_part)?;
    
    for ils_iter in 0..ils_iterations {
        
        let d_partition_restored = stream.memcpy_stod(&best_partition_host)?;
        let d_nodes_in_part_restored = stream.memcpy_stod(&best_nodes_in_part_host)?;
        d_partition = d_partition_restored;
        d_nodes_in_part = d_nodes_in_part_restored;
        
        
        let seed = 123456789u64 + (ils_iter as u64) * 987654321u64;
        unsafe {
            stream
                .launch_builder(&perturb_kernel)
                .arg(&(challenge.num_nodes as i32))
                .arg(&(challenge.num_parts as i32))
                .arg(&(challenge.max_part_size as i32))
                .arg(&perturb_strength)
                .arg(&mut d_partition)
                .arg(&mut d_nodes_in_part)
                .arg(&seed)
                .launch(one_thread_cfg.clone())?;
        }
        
        
        for _ in 0..ils_quick_refine {
            unsafe {
                stream
                    .launch_builder(&reset_counters_kernel)
                    .arg(&mut d_num_valid_moves)
                    .arg(&mut d_moves_executed)
                    .launch(one_thread_cfg.clone())?;
            }
            
            unsafe {
                stream
                    .launch_builder(&precompute_edge_flags_kernel)
                    .arg(&(challenge.num_hyperedges as i32))
                    .arg(&(challenge.num_nodes as i32))
                    .arg(&challenge.d_hyperedge_nodes)
                    .arg(&challenge.d_hyperedge_offsets)
                    .arg(&d_partition)
                    .arg(&mut d_edge_flags_all)
                    .arg(&mut d_edge_flags_double)
                    .launch(hedge_cfg.clone())?;
            }
            
            unsafe {
                stream
                    .launch_builder(&compute_moves_kernel)
                    .arg(&(challenge.num_nodes as i32))
                    .arg(&(challenge.num_parts as i32))
                    .arg(&(challenge.max_part_size as i32))
                    .arg(&challenge.d_node_hyperedges)
                    .arg(&challenge.d_node_offsets)
                    .arg(&d_partition)
                    .arg(&d_nodes_in_part)
                    .arg(&d_edge_flags_all)
                    .arg(&d_edge_flags_double)
                    .arg(&mut d_move_priorities)
                    .arg(&mut d_num_valid_moves)
                    .launch(cfg.clone())?;
            }
            
            let num_valid_moves = stream.memcpy_dtov(&d_num_valid_moves)?[0];
            if num_valid_moves == 0 {
                break;
            }
            
            let move_keys = stream.memcpy_dtov(&d_move_priorities)?;
            
            valid_moves.clear();
            for (node, &key) in move_keys.iter().enumerate() {
                if key > 0 {
                    valid_moves.push((node, key));
                }
            }
            
            if valid_moves.is_empty() {
                break;
            }
            
            let cmp = |a: &(usize, i32), b: &(usize, i32)| b.1.cmp(&a.1).then(a.0.cmp(&b.0));
            
            let mut k = valid_moves.len();
            if k > move_limit {
                k = move_limit;
                valid_moves.select_nth_unstable_by(k - 1, cmp);
                valid_moves[..k].sort_unstable_by(cmp);
            } else {
                valid_moves.sort_unstable_by(cmp);
            }
            
            sorted_move_nodes.clear();
            sorted_move_parts.clear();
            sorted_move_nodes.extend(valid_moves[..k].iter().map(|(node, _)| *node as i32));
            sorted_move_parts.extend(valid_moves[..k].iter().map(|(_, key)| (key & 63) as i32));
            
            let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
            let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;
            
            unsafe {
                stream
                    .launch_builder(&execute_moves_kernel)
                    .arg(&(sorted_move_nodes.len() as i32))
                    .arg(&d_sorted_move_nodes)
                    .arg(&d_sorted_move_parts)
                    .arg(&(challenge.max_part_size as i32))
                    .arg(&mut d_partition)
                    .arg(&mut d_nodes_in_part)
                    .arg(&mut d_moves_executed)
                    .launch(one_thread_cfg.clone())?;
            }
            
            let moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];
            if moves_executed == 0 {
                break;
            }
        }
        
        
        unsafe {
            stream
                .launch_builder(&compute_connectivity_kernel)
                .arg(&(challenge.num_hyperedges as i32))
                .arg(&challenge.d_hyperedge_nodes)
                .arg(&challenge.d_hyperedge_offsets)
                .arg(&d_partition)
                .arg(&mut d_connectivity)
                .launch(hedge_cfg.clone())?;
        }
        
        let connectivity_vec = stream.memcpy_dtov(&d_connectivity)?;
        let new_connectivity: i32 = connectivity_vec.iter().sum();
        
        
        let delta = new_connectivity - best_connectivity;
        let temperature = 1000.0 * (1.0 - (ils_iter as f64 / ils_iterations as f64)).powi(2);
        let accept_probability = if delta < 0 {
            1.0 
        } else {
            (-delta as f64 / temperature).exp()
        };
        
        let random_val = ((123456789u64 + ils_iter as u64 * 111111111u64) % 1000000) as f64 / 1000000.0;
        
        if random_val < accept_probability {
            best_connectivity = new_connectivity;
            best_partition_host = stream.memcpy_dtov(&d_partition)?;
            best_nodes_in_part_host = stream.memcpy_dtov(&d_nodes_in_part)?;
        }
    }
    
    
    let d_partition_final = stream.memcpy_stod(&best_partition_host)?;
    let d_nodes_in_part_final = stream.memcpy_stod(&best_nodes_in_part_host)?;
    d_partition = d_partition_final;
    d_nodes_in_part = d_nodes_in_part_final;
    
    
    for _ in 0..post_ils_polish {
        unsafe {
            stream
                .launch_builder(&reset_counters_kernel)
                .arg(&mut d_num_valid_moves)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }
        
        unsafe {
            stream
                .launch_builder(&precompute_edge_flags_kernel)
                .arg(&(challenge.num_hyperedges as i32))
                .arg(&(challenge.num_nodes as i32))
                .arg(&challenge.d_hyperedge_nodes)
                .arg(&challenge.d_hyperedge_offsets)
                .arg(&d_partition)
                .arg(&mut d_edge_flags_all)
                .arg(&mut d_edge_flags_double)
                .launch(hedge_cfg.clone())?;
        }
        
        unsafe {
            stream
                .launch_builder(&compute_moves_kernel)
                .arg(&(challenge.num_nodes as i32))
                .arg(&(challenge.num_parts as i32))
                .arg(&(challenge.max_part_size as i32))
                .arg(&challenge.d_node_hyperedges)
                .arg(&challenge.d_node_offsets)
                .arg(&d_partition)
                .arg(&d_nodes_in_part)
                .arg(&d_edge_flags_all)
                .arg(&d_edge_flags_double)
                .arg(&mut d_move_priorities)
                .arg(&mut d_num_valid_moves)
                .launch(cfg.clone())?;
        }
        
        let num_valid_moves = stream.memcpy_dtov(&d_num_valid_moves)?[0];
        if num_valid_moves == 0 {
            break;
        }
        
        let move_keys = stream.memcpy_dtov(&d_move_priorities)?;
        
        valid_moves.clear();
        for (node, &key) in move_keys.iter().enumerate() {
            if key > 0 {
                valid_moves.push((node, key));
            }
        }
        
        if valid_moves.is_empty() {
            break;
        }
        
        let cmp = |a: &(usize, i32), b: &(usize, i32)| b.1.cmp(&a.1).then(a.0.cmp(&b.0));
        
        let mut k = valid_moves.len();
        
        let polish_limit = 100_000;
        if k > polish_limit {
            k = polish_limit;
            valid_moves.select_nth_unstable_by(k - 1, cmp);
            valid_moves[..k].sort_unstable_by(cmp);
        } else {
            valid_moves.sort_unstable_by(cmp);
        }
        
        sorted_move_nodes.clear();
        sorted_move_parts.clear();
        sorted_move_nodes.extend(valid_moves[..k].iter().map(|(node, _)| *node as i32));
        sorted_move_parts.extend(valid_moves[..k].iter().map(|(_, key)| (key & 63) as i32));
        
        let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
        let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;
        
        unsafe {
            stream
                .launch_builder(&execute_moves_kernel)
                .arg(&(sorted_move_nodes.len() as i32))
                .arg(&d_sorted_move_nodes)
                .arg(&d_sorted_move_parts)
                .arg(&(challenge.max_part_size as i32))
                .arg(&mut d_partition)
                .arg(&mut d_nodes_in_part)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }
        
        let moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];
        if moves_executed == 0 {
            break;
        }
    }

    unsafe {
        stream
            .launch_builder(&balance_kernel)
            .arg(&(challenge.num_nodes as i32))
            .arg(&(challenge.num_parts as i32))
            .arg(&1i32)
            .arg(&(challenge.max_part_size as i32))
            .arg(&mut d_partition)
            .arg(&mut d_nodes_in_part)
            .launch(one_thread_cfg.clone())?;
    }

    let post_balance_rounds = hyperparameters
        .as_ref()
        .and_then(|p| p.get("post_refinement").and_then(|v| v.as_i64()))
        .map(|v| v.clamp(0, 128) as usize)
        .unwrap_or(base_post_balance);

    for _ in 0..post_balance_rounds {
        unsafe {
            stream
                .launch_builder(&reset_counters_kernel)
                .arg(&mut d_num_valid_moves)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }

        unsafe {
            stream
                .launch_builder(&precompute_edge_flags_kernel)
                .arg(&(challenge.num_hyperedges as i32))
                .arg(&(challenge.num_nodes as i32))
                .arg(&challenge.d_hyperedge_nodes)
                .arg(&challenge.d_hyperedge_offsets)
                .arg(&d_partition)
                .arg(&mut d_edge_flags_all)
                .arg(&mut d_edge_flags_double)
                .launch(hedge_cfg.clone())?;
        }

        unsafe {
            stream
                .launch_builder(&compute_moves_kernel)
                .arg(&(challenge.num_nodes as i32))
                .arg(&(challenge.num_parts as i32))
                .arg(&(challenge.max_part_size as i32))
                .arg(&challenge.d_node_hyperedges)
                .arg(&challenge.d_node_offsets)
                .arg(&d_partition)
                .arg(&d_nodes_in_part)
                .arg(&d_edge_flags_all)
                .arg(&d_edge_flags_double)
                .arg(&mut d_move_priorities)
                .arg(&mut d_num_valid_moves)
                .launch(cfg.clone())?;
        }
        let num_valid_moves = stream.memcpy_dtov(&d_num_valid_moves)?[0];
        if num_valid_moves == 0 {
            break;
        }

        let move_keys = stream.memcpy_dtov(&d_move_priorities)?;

        valid_moves.clear();
        for (node, &key) in move_keys.iter().enumerate() {
            if key > 0 {
                valid_moves.push((node, key));
            }
        }

        if valid_moves.is_empty() {
            break;
        }

        let cmp = |a: &(usize, i32), b: &(usize, i32)| b.1.cmp(&a.1).then(a.0.cmp(&b.0));

        let mut k = valid_moves.len();
        
        
        let adaptive_limit = move_limit / 2;
        
        if k > adaptive_limit {
            k = adaptive_limit;
            valid_moves.select_nth_unstable_by(k - 1, cmp);
            valid_moves[..k].sort_unstable_by(cmp);
        } else if k > 1000 {
            
            valid_moves.select_nth_unstable_by(k - 1, cmp);
            valid_moves[..k].sort_unstable_by(cmp);
        } else {
            
            valid_moves.sort_unstable_by(cmp);
        }

        sorted_move_nodes.clear();
        sorted_move_parts.clear();
        sorted_move_nodes.extend(valid_moves[..k].iter().map(|(node, _)| *node as i32));
        sorted_move_parts.extend(valid_moves[..k].iter().map(|(_, key)| (key & 63) as i32));

        let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
        let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;

        unsafe {
            stream
                .launch_builder(&execute_moves_kernel)
                .arg(&(sorted_move_nodes.len() as i32))
                .arg(&d_sorted_move_nodes)
                .arg(&d_sorted_move_parts)
                .arg(&(challenge.max_part_size as i32))
                .arg(&mut d_partition)
                .arg(&mut d_nodes_in_part)
                .arg(&mut d_moves_executed)
                .launch(one_thread_cfg.clone())?;
        }
        let mut moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];

        if moves_executed == 0 && k < valid_moves.len() {
            
            sorted_move_nodes.clear();
            sorted_move_parts.clear();
            for i in k..valid_moves.len() {
                sorted_move_nodes.push(valid_moves[i].0 as i32);
                sorted_move_parts.push((valid_moves[i].1 & 63) as i32);
            }

            let d_sorted_move_nodes = stream.memcpy_stod(&sorted_move_nodes)?;
            let d_sorted_move_parts = stream.memcpy_stod(&sorted_move_parts)?;

            unsafe {
                stream
                    .launch_builder(&execute_moves_kernel)
                    .arg(&(sorted_move_nodes.len() as i32))
                    .arg(&d_sorted_move_nodes)
                    .arg(&d_sorted_move_parts)
                    .arg(&(challenge.max_part_size as i32))
                    .arg(&mut d_partition)
                    .arg(&mut d_nodes_in_part)
                    .arg(&mut d_moves_executed)
                    .launch(one_thread_cfg.clone())?;
            }

            moves_executed = stream.memcpy_dtov(&d_moves_executed)?[0];
        }

        if moves_executed == 0 {
            break;
        }
    }

    let partition = stream.memcpy_dtov(&d_partition)?;
    let partition_u32: Vec<u32> = partition.iter().map(|&x| x as u32).collect();

    save_solution(&Solution {
        partition: partition_u32,
    })?;
    Ok(())
}
